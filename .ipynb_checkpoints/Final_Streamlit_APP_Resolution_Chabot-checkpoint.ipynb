{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Depedencies**"
      ],
      "metadata": {
        "id": "5c6pI0efxjqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain-groq\n",
        "%pip install --upgrade --quiet  sentence_transformers\n",
        "%pip install --upgrade --quiet  langchain-community\n",
        "%pip install --upgrade --quiet qdrant-client\n",
        "%pip install --upgrade --quiet langgraph\n",
        "%pip install --upgrade --quiet langchain-huggingface\n",
        "%pip install --upgrade --quiet streamlit\n",
        "%pip install --upgrade --quiet pyngrok\n",
        "# %pip install --upgrade transformers sentence_transformers protobuf\n"
      ],
      "metadata": {
        "id": "okDapS_IjjSQ",
        "outputId": "fb541667-231d-4851-bc85-ac51e9d65ed6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/379.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.8/379.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-aiplatform 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.3 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.27.3 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.3 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Streamlit APP Code**"
      ],
      "metadata": {
        "id": "YgDv76-7QPPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import requests\n",
        "import zipfile\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from qdrant_client import QdrantClient\n",
        "from langchain import PromptTemplate\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.vectorstores import Qdrant\n",
        "\n",
        "# Streamlit app title and description\n",
        "st.title(\"Zoning Resolution Chatbot\")\n",
        "st.write(\"Powered by LLAMA3.1 70B. Ask me anything about the Zoning Resolution!\")\n",
        "\n",
        "# Helper function to get the language model\n",
        "def get_llm():\n",
        "    \"\"\"\n",
        "    Returns the language model instance.\n",
        "\n",
        "    This function initializes and returns a ChatGroq language model configured with the specified model name,\n",
        "    temperature, maximum tokens, and other settings.\n",
        "\n",
        "    Returns:\n",
        "        ChatGroq: An instance of the ChatGroq language model.\n",
        "    \"\"\"\n",
        "    llm = ChatGroq(\n",
        "        model=\"llama-3.1-70b-versatile\",\n",
        "        temperature=0,\n",
        "        max_tokens=1024,\n",
        "        top_p=1,\n",
        "        stream=False,\n",
        "        stop=None,\n",
        "        api_key='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'\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "# Helper function to get the embeddings\n",
        "def get_embeddings():\n",
        "    \"\"\"\n",
        "    Returns the embeddings model instance.\n",
        "\n",
        "    This function initializes and returns a HuggingFaceEmbeddings model if not already present\n",
        "    in the session state. It uses the specified model name for embeddings.\n",
        "\n",
        "    Returns:\n",
        "        HuggingFaceEmbeddings: An instance of the HuggingFaceEmbeddings model.\n",
        "    \"\"\"\n",
        "    if \"embeddings\" not in st.session_state:\n",
        "        with st.spinner(\"Loading embeddings...\"):\n",
        "            st.session_state.embeddings = HuggingFaceEmbeddings(model_name=\"dunzhang/stella_en_1.5B_v5\")\n",
        "        st.success(\"Embeddings loaded successfully.\")\n",
        "    return st.session_state.embeddings\n",
        "\n",
        "# Helper function to get the vector store\n",
        "def get_vector_store():\n",
        "    \"\"\"\n",
        "    Returns the vector store instance.\n",
        "\n",
        "    This function initializes and returns a QdrantClient vector store if not already present\n",
        "    in the session state. It loads the embeddings and connects to the specified Qdrant path.\n",
        "\n",
        "    Returns:\n",
        "        Qdrant: An instance of the Qdrant vector store retriever.\n",
        "    \"\"\"\n",
        "    if \"vector_store\" not in st.session_state:\n",
        "        with st.spinner(\"Loading vector store...\"):\n",
        "            embeddings = get_embeddings()\n",
        "            client = QdrantClient(path='/content/content/local_qdrant')\n",
        "            st.session_state.vector_store = Qdrant(client=client, collection_name=\"my_documents\", embeddings=embeddings)\n",
        "        st.success(\"Vector store loaded successfully.\")\n",
        "    return st.session_state.vector_store.as_retriever()\n",
        "\n",
        "# Define the agent state class\n",
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    grades: list[str]\n",
        "    llm_output: str\n",
        "    documents: list[str]\n",
        "    on_topic: bool\n",
        "\n",
        "# Function to retrieve documents based on the question\n",
        "def retrieve_docs(state: AgentState):\n",
        "    \"\"\"\n",
        "    Retrieves documents relevant to the question.\n",
        "\n",
        "    This function retrieves documents from the vector store based on the question\n",
        "    provided in the state and updates the state with the retrieved documents' content.\n",
        "\n",
        "    Args:\n",
        "        state (AgentState): The current state containing the question and other data.\n",
        "\n",
        "    Returns:\n",
        "        AgentState: The updated state with the retrieved documents.\n",
        "    \"\"\"\n",
        "    question = state[\"question\"]\n",
        "    documents = retriever.get_relevant_documents(query=question)\n",
        "    state[\"documents\"] = [doc.page_content for doc in documents]\n",
        "    return state\n",
        "\n",
        "# Define the question grading class\n",
        "class GradeQuestion(BaseModel):\n",
        "    score: str = Field(description=\"Question is about Zoning Resolution? If yes -> 'Yes' if not -> 'No'\")\n",
        "\n",
        "# Function to classify the question\n",
        "def question_classifier(state: AgentState):\n",
        "    \"\"\"\n",
        "    Classifies the question based on its topic.\n",
        "\n",
        "    This function classifies the question as either relevant or irrelevant to the Zoning Resolution\n",
        "    topics. It uses a language model to assess the question and update the state with the classification.\n",
        "\n",
        "    Args:\n",
        "        state (AgentState): The current state containing the question and other data.\n",
        "\n",
        "    Returns:\n",
        "        AgentState: The updated state with the topic classification.\n",
        "    \"\"\"\n",
        "    question = state[\"question\"]\n",
        "    system = \"\"\"\n",
        "    You are a grader assessing the topic of a user question.\n",
        "    Only answer if the question is about one of the following topics related to zoning resolutions:\n",
        "    1. Zoning laws and regulations.\n",
        "    2. Land use planning and development.\n",
        "    3. Zoning permits and approvals.\n",
        "    4. Variances and special zoning exceptions.\n",
        "\n",
        "    Examples: What are the zoning laws for residential areas? -> Yes\n",
        "              How do I apply for a zoning variance? -> Yes\n",
        "              What is the zoning for my property? -> Yes\n",
        "              What is the capital of France? -> No\n",
        "\n",
        "    If the question IS about these topics respond with \"Yes\", otherwise respond with \"No\".\n",
        "    \"\"\"\n",
        "    grade_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"User question: {question}\"),\n",
        "    ])\n",
        "    llm = get_llm()\n",
        "    structured_llm = llm.with_structured_output(GradeQuestion)\n",
        "    grader_llm = grade_prompt | structured_llm\n",
        "    result = grader_llm.invoke({\"question\": question})\n",
        "    state[\"on_topic\"] = result.score\n",
        "    return state\n",
        "\n",
        "# Function to route based on the topic classification\n",
        "def on_topic_router(state: AgentState):\n",
        "    \"\"\"\n",
        "    Routes the flow based on the topic classification.\n",
        "\n",
        "    This function routes the state flow based on whether the question is classified as\n",
        "    on-topic or off-topic. It returns the corresponding next state identifier.\n",
        "\n",
        "    Args:\n",
        "        state (AgentState): The current state containing the topic classification.\n",
        "\n",
        "    Returns:\n",
        "        str: The next state identifier, either \"on_topic\" or \"off_topic\".\n",
        "    \"\"\"\n",
        "    on_topic = state[\"on_topic\"]\n",
        "    if on_topic.lower() == \"yes\":\n",
        "        return \"on_topic\"\n",
        "    return \"off_topic\"\n",
        "\n",
        "# Function for off-topic response\n",
        "def off_topic_response(state: AgentState):\n",
        "    \"\"\"\n",
        "    Provides a response for off-topic questions.\n",
        "\n",
        "    This function sets the response message for questions classified as off-topic\n",
        "    and updates the state with this response.\n",
        "\n",
        "    Args:\n",
        "        state (AgentState): The current state containing the question and other data.\n",
        "\n",
        "    Returns:\n",
        "        AgentState: The updated state with the off-topic response.\n",
        "    \"\"\"\n",
        "    state[\"llm_output\"] = \"I can't respond to that!\"\n",
        "    return state\n",
        "\n",
        "# Define the document grading class\n",
        "class GradeDocuments(BaseModel):\n",
        "    score: str = Field(description=\"Documents are relevant to the question, 'Yes' or 'No'\")\n",
        "\n",
        "# Function to grade documents based on their relevance\n",
        "def document_grader(state: AgentState):\n",
        "    \"\"\"\n",
        "    Grades the relevance of retrieved documents.\n",
        "\n",
        "    This function assesses the relevance of retrieved documents to the user's question\n",
        "    and updates the state with the relevance scores.\n",
        "\n",
        "    Args:\n",
        "        state (AgentState): The current state containing the documents and question.\n",
        "\n",
        "    Returns:\n",
        "        AgentState: The updated state with the relevance scores.\n",
        "    \"\"\"\n",
        "    docs = state[\"documents\"]\n",
        "    question = state[\"question\"]\n",
        "    system = \"\"\"\n",
        "    You are a grader assessing relevance of a retrieved document to a user question.\n",
        "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
        "    Give a binary score 'Yes' or 'No' score to indicate whether the document is relevant to the question.\n",
        "    \"\"\"\n",
        "    grade_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ])\n",
        "    llm = get_llm()\n",
        "    structured_llm = llm.with_structured_output(GradeDocuments)\n",
        "    grader_llm = grade_prompt | structured_llm\n",
        "    scores = []\n",
        "    for doc in docs:\n",
        "        result = grader_llm.invoke({\"document\": doc, \"question\": question})\n",
        "        scores.append(result.score)\n",
        "    state[\"grades\"] = scores\n",
        "    return state\n",
        "\n",
        "# Function to route based on document grades\n",
        "def gen_router(state: AgentState):\n",
        "    \"\"\"\n",
        "    Routes the flow based on document relevance grades.\n",
        "\n",
        "    This function determines the next action based on the relevance grades of the documents.\n",
        "    If any document is relevant, it proceeds to generate an answer; otherwise, it rewrites the query.\n",
        "\n",
        "    Args:\n",
        "        state (AgentState): The current state containing the document grades.\n",
        "\n",
        "    Returns:\n",
        "        str: The next state identifier, either \"generate\" or \"rewrite_query\".\n",
        "    \"\"\"\n",
        "    grades = state[\"grades\"]\n",
        "    if any(grade.lower() == \"yes\" for grade in grades):\n",
        "        return \"generate\"\n",
        "    return \"rewrite_query\"\n",
        "\n",
        "# Function to rewrite the query for better results\n",
        "def rewriter(state: AgentState):\n",
        "    \"\"\"\n",
        "    Rewrites the user's question for better results.\n",
        "\n",
        "    This function rewrites the initial question to improve its clarity and relevance for better retrieval.\n",
        "    It updates the state with the rewritten question.\n",
        "\n",
        "    Args:\n",
        "        state (AgentState): The current state containing the initial question.\n",
        "\n",
        "    Returns:\n",
        "        AgentState: The updated state with the rewritten question.\n",
        "    \"\"\"\n",
        "    question = state[\"question\"]\n",
        "    system = \"\"\"\n",
        "    You are a question re-writer that converts an input question to a better version that is optimized for retrieval.\n",
        "    Look at the input and try to reason about the underlying semantic intent/meaning.\n",
        "    \"\"\"\n",
        "    re_write_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\"),\n",
        "    ])\n",
        "    llm = get_llm()\n",
        "    question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "    output = question_rewriter.invoke({\"question\": question})\n",
        "    state[\"question\"] = output\n",
        "    return state\n",
        "\n",
        "# Function to generate the final answer\n",
        "def generate_answer(state: AgentState):\n",
        "    \"\"\"\n",
        "    Generates the final answer based on the context and question.\n",
        "\n",
        "    This function generates an answer to the user's question using the language model and\n",
        "    the retrieved documents as context. It updates the state with the generated answer.\n",
        "\n",
        "    Args:\n",
        "        state (AgentState): The current state containing the question and context.\n",
        "\n",
        "    Returns:\n",
        "        AgentState: The updated state with the generated answer.\n",
        "    \"\"\"\n",
        "    llm = get_llm()\n",
        "    question = state[\"question\"]\n",
        "    context = state[\"documents\"]\n",
        "    template = \"\"\"Answer the question based only on the following context:\n",
        "    {context}\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template=template)\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    result = chain.invoke({\"question\": question, \"context\": context})\n",
        "    state[\"llm_output\"] = result\n",
        "    return state\n",
        "\n",
        "# Define the workflow state graph\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"topic_decision\", question_classifier)\n",
        "workflow.add_node(\"off_topic_response\", off_topic_response)\n",
        "workflow.add_node(\"retrieve_docs\", retrieve_docs)\n",
        "workflow.add_node(\"rewrite_query\", rewriter)\n",
        "workflow.add_node(\"generate_answer\", generate_answer)\n",
        "workflow.add_node(\"document_grader\", document_grader)\n",
        "workflow.add_edge(\"off_topic_response\", END)\n",
        "workflow.add_edge(\"retrieve_docs\", \"document_grader\")\n",
        "workflow.add_conditional_edges(\"topic_decision\", on_topic_router, {\n",
        "    \"on_topic\": \"retrieve_docs\",\n",
        "    \"off_topic\": \"off_topic_response\",\n",
        "})\n",
        "workflow.add_conditional_edges(\"document_grader\", gen_router, {\n",
        "    \"generate\": \"generate_answer\",\n",
        "    \"rewrite_query\": \"rewrite_query\",\n",
        "})\n",
        "workflow.add_edge(\"rewrite_query\", \"retrieve_docs\")\n",
        "workflow.add_edge(\"generate_answer\", END)\n",
        "workflow.set_entry_point(\"topic_decision\")\n",
        "\n",
        "# Compile the workflow\n",
        "app = workflow.compile()\n",
        "\n",
        "# Download and prepare the document collection once\n",
        "if \"documents_prepared\" not in st.session_state:\n",
        "    with st.spinner(\"Downloading and preparing document collection...\"):\n",
        "        url = 'https://drive.usercontent.google.com/u/0/uc?id=1Lufl_FhsCGxL_2txB10wI1mE4rESMPGF&export=download'\n",
        "        output_path = 'downloaded_file.zip'\n",
        "\n",
        "        # Download the file\n",
        "        response = requests.get(url)\n",
        "        with open(output_path, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "\n",
        "        # Unzip the file\n",
        "        with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall('content')\n",
        "\n",
        "        # Optionally, remove the downloaded zip file if it's no longer needed\n",
        "        os.remove(output_path)\n",
        "\n",
        "        st.session_state.documents_prepared = True\n",
        "    st.success(\"Documents prepared successfully.\")\n",
        "\n",
        "# Load embeddings and vector store\n",
        "embeddings = get_embeddings()\n",
        "retriever = get_vector_store()\n",
        "\n",
        "# Streamlit UI for chat interaction\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display previous messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Input field for user question\n",
        "if prompt := st.chat_input(\"Ask about Zoning Resolution:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Initialize state and invoke workflow\n",
        "    state = {\"question\": prompt}\n",
        "    result = app.invoke(state)\n",
        "    full_response = result[\"llm_output\"]\n",
        "\n",
        "    # Display response from the assistant\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(full_response)\n",
        "\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n"
      ],
      "metadata": {
        "id": "EEDMHAj5EhLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d7b68b-b308-47de-f368-7409370cb2d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "qLfDI6qxj66l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set ngrok authentication token\n",
        "ngrok.set_auth_token(\"2jnUMJlaqO1OdKVdYTkHhJIfqwN_wGgVydz9grPWeaVWF1cQ\")  # Replace with your ngrok auth token if necessary\n",
        "\n",
        "# Start the Streamlit app in the background\n",
        "!nohup streamlit run app.py --server.port 1850 &\n",
        "\n",
        "# Establish the ngrok tunnel\n",
        "public_url = ngrok.connect(1850)\n",
        "print(public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNSsGrjXj7sy",
        "outputId": "75358517-8b44-40d6-e692-9745ba3f9b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "NgrokTunnel: \"https://f07f-35-185-253-76.ngrok-free.app\" -> \"http://localhost:1850\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "tunnels = ngrok.get_tunnels()\n",
        "tunnels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1bWiP7Qj8oS",
        "outputId": "4598a9b4-35dd-4f46-9f27-57aae05248dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<NgrokTunnel: \"https://f07f-35-185-253-76.ngrok-free.app\" -> \"http://localhost:1850\">]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Example Questions**"
      ],
      "metadata": {
        "id": "o6drXrBfQFgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "What are Special Permit for Cogeneration Power Plant\n",
        "\n",
        "What is the Maximum Floor Area Ratio for Community Facilities\n",
        "\n",
        "what are the FLUSHING WATERFRONT ACCESS PLAN with specific block and lot numbers\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "FjV-OnE0j-ed"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}